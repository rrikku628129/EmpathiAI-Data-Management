{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff22737-89e7-4a78-bc01-ecbb320f4c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, numpy as np, pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "CSV_PATH   = \"mental_health_counseling_conversations_rated.csv\"  \n",
    "OUT_DIR    = \"data/processed\"\n",
    "REPORT_DIR = \"reports\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "RATING_COLS = [\"avg_empathy_score\", \"avg_appropriateness_score\", \"avg_relevance_score\"]\n",
    "\n",
    "\n",
    "BASE_TEXT_COLS = [\"context\", \"response\"]\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47918bc4-83c2-4e3a-b256-eb0e655f4d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import regex as re2\n",
    "except Exception:\n",
    "    re2 = re\n",
    "\n",
    "try:\n",
    "    from ftfy import fix_text\n",
    "except Exception:\n",
    "    def fix_text(x): return x\n",
    "\n",
    "try:\n",
    "    from unidecode import unidecode\n",
    "except Exception:\n",
    "    def unidecode(x): return x\n",
    "\n",
    "RE_URL        = re2.compile(r\"(https?://\\S+|www\\.\\S+)\", flags=re2.IGNORECASE)\n",
    "RE_EMAIL      = re2.compile(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")\n",
    "RE_PHONE      = re2.compile(r\"(?:(?:\\+?\\d{1,3}[-.\\s]*)?(?:\\(?\\d{2,4}\\)?[-.\\s]*)?\\d{3}[-.\\s]?\\d{4})\")\n",
    "RE_MENTION    = re2.compile(r\"(?<=\\s|^)@\\w+\")\n",
    "RE_HASHTAG    = re2.compile(r\"(?<=\\s|^)#\\w+\")\n",
    "RE_WHITESPACE = re2.compile(r\"\\s+\")\n",
    "try:\n",
    "    RE_EMOJI = re2.compile(r\"[\\p{So}\\p{Sk}\\p{Cs}\\p{Cn}\\U00010000-\\U0010FFFF]+\", flags=re2.UNICODE)\n",
    "except Exception:\n",
    "    RE_EMOJI = None\n",
    "\n",
    "def basic_sanitize(text: str, remove_emoji=True, lowercase=True) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = fix_text(text)\n",
    "    text = unidecode(text)\n",
    "    text = RE_URL.sub(\" \", text)\n",
    "    text = RE_EMAIL.sub(\" \", text)\n",
    "    text = RE_PHONE.sub(\" [PHONE] \", text)\n",
    "    text = RE_MENTION.sub(\" \", text)\n",
    "    text = RE_HASHTAG.sub(\" \", text)\n",
    "    if RE_EMOJI is not None and remove_emoji:\n",
    "        text = RE_EMOJI.sub(\" \", text)\n",
    "    text = RE_WHITESPACE.sub(\" \", text).strip()\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11812bdd-1937-461f-bac7-d8170eb03aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import regex as re2\n",
    "except Exception:\n",
    "    re2 = re\n",
    "\n",
    "try:\n",
    "    from ftfy import fix_text\n",
    "except Exception:\n",
    "    def fix_text(x): return x\n",
    "\n",
    "try:\n",
    "    from unidecode import unidecode\n",
    "except Exception:\n",
    "    def unidecode(x): return x\n",
    "\n",
    "RE_URL = re2.compile(r\"(https?://\\S+|www\\.\\S+)\", re2.IGNORECASE)\n",
    "RE_EMAIL = re2.compile(r\"[A-Za-z0-9_.+-]+@[A-Za-z0-9-]+\\.[A-Za-z0-9-.]+\")\n",
    "RE_PHONE = re2.compile(r\"(?:(?:\\+?\\d{1,3}[-.\\s]*)?(?:\\(?\\d{2,4}\\)?[-.\\s]*)?\\d{3}[-.\\s]?\\d{4})\")\n",
    "RE_MENTION = re2.compile(r\"(?<=\\s|^)@\\w+\")\n",
    "RE_HASHTAG = re2.compile(r\"(?<=\\s|^)#\\w+\")\n",
    "RE_WS = re2.compile(r\"\\s+\")\n",
    "try:\n",
    "    RE_EMOJI = re2.compile(r\"[\\p{So}\\p{Sk}\\p{Cs}\\p{Cn}\\U00010000-\\U0010FFFF]+\", re2.UNICODE)\n",
    "except Exception:\n",
    "    RE_EMOJI = None\n",
    "\n",
    "def basic_sanitize(text: str, lower: bool = True, rm_emoji: bool = True) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = fix_text(text)\n",
    "    text = unidecode(text)\n",
    "    text = RE_URL.sub(\" \", text)\n",
    "    text = RE_EMAIL.sub(\" \", text)\n",
    "    text = RE_PHONE.sub(\" [PHONE] \", text)\n",
    "    text = RE_MENTION.sub(\" \", text)\n",
    "    text = RE_HASHTAG.sub(\" \", text)\n",
    "    if rm_emoji and RE_EMOJI:\n",
    "        text = RE_EMOJI.sub(\" \", text)\n",
    "    text = RE_WS.sub(\" \", text).strip()\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e15824-ba68-43f9-ab36-1ff97d6d1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Patch: redefine basic_sanitize with expected keyword args ===\n",
    "import re\n",
    "\n",
    "try:\n",
    "    import regex as re2\n",
    "except Exception:\n",
    "    re2 = re\n",
    "\n",
    "try:\n",
    "    from ftfy import fix_text\n",
    "except Exception:\n",
    "    def fix_text(x): return x\n",
    "\n",
    "try:\n",
    "    from unidecode import unidecode\n",
    "except Exception:\n",
    "    def unidecode(x): return x\n",
    "\n",
    "# compile patterns inside this cell so it's self-contained\n",
    "RE_URL        = re2.compile(r\"(https?://\\S+|www\\.\\S+)\", flags=re2.IGNORECASE)\n",
    "RE_EMAIL      = re2.compile(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")\n",
    "RE_PHONE      = re2.compile(r\"(?:(?:\\+?\\d{1,3}[-.\\s]*)?(?:\\(?\\d{2,4}\\)?[-.\\s]*)?\\d{3}[-.\\s]?\\d{4})\")\n",
    "RE_MENTION    = re2.compile(r\"(?<=\\s|^)@\\w+\")\n",
    "RE_HASHTAG    = re2.compile(r\"(?<=\\s|^)#\\w+\")\n",
    "RE_WHITESPACE = re2.compile(r\"\\s+\")\n",
    "try:\n",
    "    RE_EMOJI = re2.compile(r\"[\\p{So}\\p{Sk}\\p{Cs}\\p{Cn}\\U00010000-\\U0010FFFF]+\", flags=re2.UNICODE)\n",
    "except Exception:\n",
    "    RE_EMOJI = None\n",
    "\n",
    "def basic_sanitize(text: str, remove_emoji: bool = True, lowercase: bool = True) -> str:\n",
    "    \"\"\"Sanitize text with optional emoji removal and lowercasing.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = fix_text(text)\n",
    "    t = unidecode(t)\n",
    "    t = RE_URL.sub(\" \", t)\n",
    "    t = RE_EMAIL.sub(\" \", t)\n",
    "    t = RE_PHONE.sub(\" [PHONE] \", t)\n",
    "    t = RE_MENTION.sub(\" \", t)\n",
    "    t = RE_HASHTAG.sub(\" \", t)\n",
    "    if remove_emoji and RE_EMOJI is not None:\n",
    "        t = RE_EMOJI.sub(\" \", t)\n",
    "    t = RE_WHITESPACE.sub(\" \", t).strip()\n",
    "    if lowercase:\n",
    "        t = t.lower()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "414a5226-18f1-4664-9630-0ec9e8a25f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['index', 'context', 'response', 'empathy_llama-3-2-1b', 'empathy_llama-3-2-3b', 'empathy_llama-3-1-8b', 'empathy_qwen-2-5-7b', 'appropriateness_llama-3-2-1b', 'appropriateness_llama-3-2-3b', 'appropriateness_llama-3-1-8b', 'appropriateness_qwen-2-5-7b', 'relevance_llama-3-2-1b', 'relevance_llama-3-2-3b', 'relevance_llama-3-1-8b', 'relevance_qwen-2-5-7b', 'explanation_llama-3-2-1b', 'explanation_llama-3-2-3b', 'explanation_llama-3-1-8b', 'explanation_qwen-2-5-7b', 'generated_text_llama-3-2-1b', 'generated_text_llama-3-2-3b', 'generated_text_llama-3-1-8b', 'generated_text_qwen-2-5-7b', 'avg_empathy_score', 'avg_appropriateness_score', 'avg_relevance_score']\n",
      "Shape before: (3512, 26)\n",
      "Filtered 1483 rows; remain: 2029\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>avg_empathy_score</th>\n",
       "      <th>avg_appropriateness_score</th>\n",
       "      <th>avg_relevance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i'm going through some things with my feelings...</td>\n",
       "      <td>224</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm going through some things with my feelings...</td>\n",
       "      <td>429</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm going through some things with my feelings...</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  text_len  \\\n",
       "0  i'm going through some things with my feelings...       224   \n",
       "1  i'm going through some things with my feelings...       429   \n",
       "2  i'm going through some things with my feelings...       121   \n",
       "\n",
       "   avg_empathy_score  avg_appropriateness_score  avg_relevance_score  \n",
       "0                2.5                       2.75                 4.25  \n",
       "1                4.5                       5.00                 4.75  \n",
       "2                3.0                       4.25                 3.25  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"Shape before:\", df.shape)\n",
    "\n",
    "for c in RATING_COLS:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# 构造 text：context + response\n",
    "def build_text(row):\n",
    "    parts = []\n",
    "    for c in BASE_TEXT_COLS:\n",
    "        if c in row and isinstance(row[c], str) and row[c].strip():\n",
    "            parts.append(row[c].strip())\n",
    "    return \" [CTX] \".join(parts) if parts else \"\"\n",
    "\n",
    "df[\"text\"] = df.apply(build_text, axis=1)\n",
    "\n",
    "\n",
    "df[\"text\"] = df[\"text\"].map(lambda x: basic_sanitize(x, remove_emoji=True, lowercase=True))\n",
    "df[\"text_len\"] = df[\"text\"].astype(str).str.split().apply(len)\n",
    "\n",
    "\n",
    "before = len(df)\n",
    "df = df[df[\"text\"].str.strip()!=\"\"]\n",
    "df = df[df[\"text_len\"]>=2]\n",
    "df = df.drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n",
    "print(f\"Filtered {before - len(df)} rows; remain: {len(df)}\")\n",
    "\n",
    "df[[\"text\",\"text_len\"] + [c for c in RATING_COLS if c in df.columns]].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1580132-734d-4239-b4ca-4343551a0ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sizes: 1623 203 203\n",
      "Saved to data/processed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "BIN_COL = None\n",
    "if \"avg_empathy_score\" in df.columns:\n",
    "    df[\"emp_bin\"] = pd.qcut(df[\"avg_empathy_score\"], q=3, labels=[0,1,2], duplicates=\"drop\")\n",
    "    BIN_COL = \"emp_bin\"\n",
    "\n",
    "train, tv = train_test_split(\n",
    "    df, test_size=0.2, random_state=SEED,\n",
    "    stratify=df[BIN_COL] if BIN_COL else None\n",
    ")\n",
    "val, test = train_test_split(\n",
    "    tv, test_size=0.5, random_state=SEED,\n",
    "    stratify=tv[BIN_COL] if BIN_COL else None\n",
    ")\n",
    "\n",
    "print(\"sizes:\", len(train), len(val), len(test))\n",
    "\n",
    "\n",
    "train.to_csv(os.path.join(OUT_DIR, \"train.csv\"), index=False)\n",
    "val.to_csv(os.path.join(OUT_DIR, \"val.csv\"), index=False)\n",
    "test.to_csv(os.path.join(OUT_DIR, \"test.csv\"), index=False)\n",
    "print(\"Saved to\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79a67523-e722-4c9e-b418-54684ecc0572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                      metric  train_mean    val_mean\n",
       " 0          avg_empathy_score    3.444855    3.357143\n",
       " 1  avg_appropriateness_score    3.959489    3.859606\n",
       " 2        avg_relevance_score    4.072089    4.018473\n",
       " 3                   text_len  230.064695  221.266010,\n",
       "                       metric     low     high  outlier_share\n",
       " 0          avg_empathy_score   1.500    5.500       0.008010\n",
       " 1  avg_appropriateness_score   1.625    6.625       0.012323\n",
       " 2        avg_relevance_score   2.250    6.250       0.033888\n",
       " 3                   text_len -76.500  495.500       0.051756,\n",
       " {'avg_empathy_score': 0.2909456262837639, 'text_len': 0.3134243942218427},\n",
       " 0.7731378309976039)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def iqr_outlier_share(s: pd.Series):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "    if s.empty: return np.nan, np.nan, np.nan\n",
    "    q1, q3 = s.quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    low  = q1 - 1.5 * iqr\n",
    "    high = q3 + 1.5 * iqr\n",
    "    share = ((s < low) | (s > high)).mean()\n",
    "    return float(low), float(high), float(share)\n",
    "\n",
    "def ks_pvalue(a: pd.Series, b: pd.Series):\n",
    "    a = pd.to_numeric(a, errors=\"coerce\").dropna()\n",
    "    b = pd.to_numeric(b, errors=\"coerce\").dropna()\n",
    "    if len(a) < 50 or len(b) < 50:\n",
    "        return np.nan\n",
    "    stat, p = stats.ks_2samp(a, b)\n",
    "    return float(p)\n",
    "\n",
    "# 缺失率\n",
    "cols_to_check = [\"text\"] + [c for c in RATING_COLS if c in train.columns]\n",
    "miss_train = train[cols_to_check].isna().mean()\n",
    "miss_val   = val[cols_to_check].isna().mean()\n",
    "\n",
    "# 描述统计 & IQR\n",
    "desc_rows, iqr_rows = [], []\n",
    "for c in [x for x in RATING_COLS if x in train.columns] + [\"text_len\"]:\n",
    "    if c in train.columns and c in val.columns:\n",
    "        desc_rows.append({\n",
    "            \"metric\": c,\n",
    "            \"train_mean\": pd.to_numeric(train[c], errors=\"coerce\").mean(),\n",
    "            \"val_mean\": pd.to_numeric(val[c], errors=\"coerce\").mean()\n",
    "        })\n",
    "        low, high, share = iqr_outlier_share(train[c])\n",
    "        iqr_rows.append({\"metric\": c, \"low\": low, \"high\": high, \"outlier_share\": share})\n",
    "\n",
    "desc_df = pd.DataFrame(desc_rows)\n",
    "iqr_df  = pd.DataFrame(iqr_rows)\n",
    "\n",
    "\n",
    "ks = {}\n",
    "for c in [\"avg_empathy_score\", \"text_len\"]:\n",
    "    if c in train.columns and c in val.columns:\n",
    "        ks[c] = ks_pvalue(train[c], val[c])\n",
    "\n",
    "# Rater consistency\n",
    "empathy_cols = [c for c in df.columns if c.startswith(\"empathy_\") and c != \"avg_empathy_score\"]\n",
    "def cronbach_alpha(df_sub: pd.DataFrame):\n",
    "    df_sub = df_sub.dropna(axis=0, how=\"any\")\n",
    "    if df_sub.shape[1] < 2 or df_sub.shape[0] == 0:\n",
    "        return np.nan\n",
    "    k = df_sub.shape[1]\n",
    "    variances = df_sub.var(axis=0, ddof=1)\n",
    "    total_var = df_sub.sum(axis=1).var(ddof=1)\n",
    "    if total_var == 0:\n",
    "        return np.nan\n",
    "    return float((k / (k - 1)) * (1 - variances.sum() / total_var))\n",
    "\n",
    "alpha_empathy = cronbach_alpha(train[empathy_cols]) if len(empathy_cols) >= 2 else np.nan\n",
    "\n",
    "desc_df, iqr_df, ks, alpha_empathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2d9ea81-32c0-461f-8496-9482858128c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Trustworthiness Report\n",
       "\n",
       "This report summarizes data trustworthiness checks: missingness, distribution shifts, outliers, and rater consistency.\n",
       "\n",
       "## Missingness\n",
       "**Train**\n",
       "- text: 0.000\n",
       "- avg_empathy_score: 0.000\n",
       "- avg_appropriateness_score: 0.000\n",
       "- avg_relevance_score: 0.000\n",
       "\n",
       "**Val**\n",
       "- text: 0.000\n",
       "- avg_empathy_score: 0.000\n",
       "- avg_appropriateness_score: 0.000\n",
       "- avg_relevance_score: 0.000\n",
       "\n",
       "## Descriptive Statistics (train vs. val)\n",
       "\n",
       "- avg_empathy_score: train_mean=3.445, val_mean=3.357\n",
       "- avg_appropriateness_score: train_mean=3.959, val_mean=3.860\n",
       "- avg_relevance_score: train_mean=4.072, val_mean=4.018\n",
       "- text_len: train_mean=230.065, val_mean=221.266\n",
       "\n",
       "## IQR Outlier Share (train)\n",
       "\n",
       "- avg_empathy_score: bounds=(1.500, 5.500), outlier_share=0.008\n",
       "- avg_appropriateness_score: bounds=(1.625, 6.625), outlier_share=0.012\n",
       "- avg_relevance_score: bounds=(2.250, 6.250), outlier_share=0.034\n",
       "- text_len: bounds=(-76.500, 495.500), outlier_share=0.052\n",
       "\n",
       "## Drift Check (KS test p-values)\n",
       "\n",
       "- avg_empathy_score: p=0.2909456262837639 → OK\n",
       "- text_len: p=0.3134243942218427 → OK\n",
       "\n",
       "## Rater Consistency (Cronbach’s α)\n",
       "\n",
       "- empathy_* internal consistency (alpha) = 0.773\n",
       "\n",
       "---\n",
       "*Optional integrations:* Evidently (drift dashboards), AIF360 (fairness metrics) can be added if installed.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] wrote: reports\\trustworthiness_report.md\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "lines = []\n",
    "lines.append(\"# Trustworthiness Report\\n\")\n",
    "lines.append(\"This report summarizes data trustworthiness checks: missingness, distribution shifts, outliers, and rater consistency.\\n\")\n",
    "\n",
    "lines.append(\"## Missingness\\n**Train**\")\n",
    "lines += [f\"- {k}: {v:.3f}\" for k, v in miss_train.items()]\n",
    "lines.append(\"\\n**Val**\")\n",
    "lines += [f\"- {k}: {v:.3f}\" for k, v in miss_val.items()]\n",
    "\n",
    "lines.append(\"\\n## Descriptive Statistics (train vs. val)\\n\")\n",
    "for _, r in desc_df.iterrows():\n",
    "    lines.append(f\"- {r['metric']}: train_mean={r['train_mean']:.3f}, val_mean={r['val_mean']:.3f}\")\n",
    "\n",
    "lines.append(\"\\n## IQR Outlier Share (train)\\n\")\n",
    "for _, r in iqr_df.iterrows():\n",
    "    lines.append(f\"- {r['metric']}: bounds=({r['low']:.3f}, {r['high']:.3f}), outlier_share={r['outlier_share']:.3f}\")\n",
    "\n",
    "lines.append(\"\\n## Drift Check (KS test p-values)\\n\")\n",
    "for c, p in ks.items():\n",
    "    flag = \"drift suspected\" if (isinstance(p, float) and p < 0.05) else \"OK\"\n",
    "    lines.append(f\"- {c}: p={p if not np.isnan(p) else 'NA'} → {flag}\")\n",
    "\n",
    "lines.append(\"\\n## Rater Consistency (Cronbach’s α)\\n\")\n",
    "if np.isnan(alpha_empathy):\n",
    "    lines.append(\"- empathy_* columns insufficient; α = NA\")\n",
    "else:\n",
    "    lines.append(f\"- empathy_* internal consistency (alpha) = {alpha_empathy:.3f}\")\n",
    "\n",
    "lines.append(\"\\n---\\n*Optional integrations:* Evidently (drift dashboards), AIF360 (fairness metrics) can be added if installed.\\n\")\n",
    "\n",
    "report_md = \"\\n\".join(lines)\n",
    "\n",
    "with open(os.path.join(REPORT_DIR, \"trustworthiness_report.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report_md)\n",
    "\n",
    "display(Markdown(report_md))\n",
    "print(\"[OK] wrote:\", os.path.join(REPORT_DIR, \"trustworthiness_report.md\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
